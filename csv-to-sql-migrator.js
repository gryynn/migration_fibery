#!/usr/bin/env node
/**
 * ========================================================================
 * MIGRATEUR CSV â†’ SQL - Fibery vers Supabase/PostgreSQL
 * ========================================================================
 *
 * GUIDE D'UTILISATION RAPIDE :
 * ===========================
 *
 * 1. PRÃ‰PARATION :
 *    - Exportez vos bases Fibery en CSV (format Excel)
 *    - Placez les fichiers dans un dossier "Important"
 *    - Chaque sous-dossier = une base de donnÃ©es Fibery
 *    - Chaque CSV = une table dans cette base
 *
 * 2. CONFIGURATION :
 *    - Modifiez importantDir avec le chemin vers votre dossier Important
 *    - Ajustez schema, batchSize selon vos besoins
 *    - Le script gÃ©nÃ¨re automatiquement les types PostgreSQL
 *
 * 3. EXÃ‰CUTION :
 *    node csv-to-sql-migrator.js
 *
 * 4. RÃ‰SULTAT :
 *    - Fichier migration-complete.sql gÃ©nÃ©rÃ©
 *    - ExÃ©cutez-le dans Supabase SQL Editor
 *
 * EXEMPLE DE STRUCTURE DE DOSSIERS ATTENDUE :
 * ===========================================
 * Important/
 *   â”œâ”€â”€ Actionalisation-Dopa/
 *   â”‚   â””â”€â”€ Actionalisation-Dopa.csv
 *   â”œâ”€â”€ PSM-Centres d'intÃ©rÃªt/
 *   â”‚   â””â”€â”€ PSM-Centres d'intÃ©rÃªt.csv
 *   â””â”€â”€ Bibliotheque-Auteurs/
 *       â””â”€â”€ Bibliotheque-Auteurs.csv
 *
 * TYPES DE DONNÃ‰ES DETECTÃ‰S AUTOMATIQUEMENT :
 * ============================================
 * - UUID : pour les IDs Fibery (format standard UUID)
 * - INTEGER : nombres entiers
 * - NUMERIC(12,2) : nombres dÃ©cimaux
 * - BOOLEAN : true/false, 1/0, yes/no
 * - DATE : format YYYY-MM-DD
 * - TIMESTAMPTZ : dates avec heure (ISO 8601)
 * - TEXT : texte par dÃ©faut
 */

const fs = require('fs');
const path = require('path');

// ============================================
// CONFIGURATION - Ã€ MODIFIER SELON VOS BESOINS
// ============================================
const CONFIG = {
  /**
   * ğŸ“ Chemin vers le dossier "Important" de votre export Fibery
   * =======================================================
   * Exemple Windows: 'C:\\Users\\nom\\Downloads\\monfichier.fibery.io_20251201\\Important'
   * Exemple Mac/Linux: '/Users/nom/Downloads/monfichier.fibery.io_20251201/Important'
   */
  importantDir: 'C:\\Users\\marti\\Downloads\\martunvert.fibery.io_20251023104856287\\Important',

  /**
   * ğŸ“„ Nom du fichier SQL qui sera gÃ©nÃ©rÃ©
   */
  outputSQL: './migration-complete.sql',

  /**
   * ğŸ—‚ï¸ Nom du schÃ©ma PostgreSQL/Supabase (sera crÃ©Ã© automatiquement)
   * Changez si vous voulez un autre nom que 'psm_root'
   */
  schema: 'psm_root',

  /**
   * ğŸ“¦ Taille des batches pour les INSERT (ajustez selon la mÃ©moire)
   * - 100: sÃ»r mais lent
   * - 500: bon Ã©quilibre
   * - 1000: rapide mais peut planter avec beaucoup de donnÃ©es
   */
  batchSize: 100,

  /**
   * ğŸ” Options avancÃ©es
   */
  options: {
    /**
     * Supprimer les tables existantes avant crÃ©ation (true = recommandÃ©)
     */
    dropExistingTables: true,

    /**
     * Ajouter des commentaires explicatifs dans le SQL
     */
    addComments: true,

    /**
     * CrÃ©er les index automatiquement (peut Ãªtre lent sur gros volumes)
     */
    createIndexes: false
  }
};

// ============================================
// FONCTIONS UTILITAIRES
// ============================================

/**
 * ğŸ¯ MOTS RÃ‰SERVÃ‰S POSTGRESQL
 * =============================
 * Liste complÃ¨te des mots-clÃ©s rÃ©servÃ©s par PostgreSQL.
 * Si un nom de colonne correspond Ã  l'un de ces mots,
 * un underscore sera ajoutÃ© devant pour Ã©viter les erreurs SQL.
 */
const RESERVED_KEYWORDS = new Set([
  'all', 'analyse', 'analyze', 'and', 'any', 'array', 'as', 'asc', 'asymmetric',
  'authorization', 'binary', 'both', 'case', 'cast', 'check', 'collate', 'collation',
  'column', 'concurrently', 'constraint', 'create', 'cross', 'current_catalog',
  'current_date', 'current_role', 'current_schema', 'current_timestamp', 'current_user',
  'default', 'deferrable', 'desc', 'distinct', 'do', 'else', 'end', 'except', 'false',
  'fetch', 'for', 'foreign', 'freeze', 'from', 'full', 'grant', 'group', 'having',
  'ilike', 'in', 'initially', 'inner', 'intersect', 'into', 'is', 'isnull', 'join',
  'lateral', 'leading', 'left', 'like', 'limit', 'localtime', 'localtimestamp',
  'natural', 'not', 'notnull', 'null', 'offset', 'on', 'only', 'or', 'order', 'outer',
  'overlaps', 'placing', 'primary', 'references', 'returning', 'right', 'select',
  'session_user', 'similar', 'some', 'symmetric', 'table', 'tablesample', 'then',
  'to', 'trailing', 'true', 'union', 'unique', 'user', 'using', 'variadic', 'verbose',
  'when', 'where', 'window', 'with'
]);

/**
 * ğŸ§¹ NETTOYAGE ET NORMALISATION DES NOMS
 * ======================================
 * Transforme les noms de colonnes/tables Fibery en noms PostgreSQL valides
 *
 * TRANSFORMATIONS APPLIQUÃ‰ES :
 * - Ã©, Ã , Ã§, etc. â†’ e, a, c (suppression des accents)
 * - "L'important" â†’ "l_important" (apostrophes)
 * - "Ma-super colonne" â†’ "ma_super_colonne" (espaces/tirets)
 * - "2nd_colonne" â†’ "_2nd_colonne" (commence par chiffre)
 * - "order" â†’ "_order" (mot rÃ©servÃ© PostgreSQL)
 * - Maximum 63 caractÃ¨res (limite PostgreSQL)
 *
 * @param {string} name - Nom original de la colonne/table
 * @returns {string} - Nom nettoyÃ© compatible PostgreSQL
 */
function sanitizeName(name) {
  if (!name) return 'unnamed';

  // Ã‰tape 1: Normalisation Unicode (dÃ©compose les caractÃ¨res accentuÃ©s)
  let clean = name
    .normalize('NFD')
    .replace(/[\u0300-\u036f]/g, '');

  // Ã‰tape 2: Remplacer les caractÃ¨res problÃ©matiques par underscore
  clean = clean.replace(/[''`]/g, '_');  // Apostrophes
  clean = clean.replace(/[\s\-\./]+/g, '_');  // Espaces, tirets, points, slashes

  // Ã‰tape 3: Garder seulement les caractÃ¨res alphanumÃ©riques + underscore
  clean = clean.replace(/[^a-zA-Z0-9_]/g, '');

  // Ã‰tape 4: Minuscules
  clean = clean.toLowerCase();

  // Ã‰tape 5: Nettoyer les underscores
  clean = clean.replace(/_+/g, '_');  // Multiples â†’ simple
  clean = clean.replace(/^_+|_+$/g, '');  // DÃ©but/fin

  // Ã‰tape 6: VÃ©rifications spÃ©ciales
  if (!clean) clean = 'unnamed';  // Si vide aprÃ¨s nettoyage
  if (/^[0-9]/.test(clean)) clean = '_' + clean;  // Commence par chiffre
  if (RESERVED_KEYWORDS.has(clean)) clean = '_' + clean;  // Mot rÃ©servÃ©

  // Ã‰tape 7: Limite de longueur PostgreSQL
  if (clean.length > 63) {
    clean = clean.substring(0, 63);
  }

  return clean;
}

/**
 * ğŸ”® DÃ‰TECTION AUTOMATIQUE DES TYPES POSTGRESQL
 * =============================================
 * Analyse une valeur et dÃ©termine le type PostgreSQL le plus appropriÃ©
 *
 * ALGORITHME DE DÃ‰TECTION :
 * ========================
 * 1. Valeurs nulles/vides â†’ TEXT (par dÃ©faut)
 * 2. Formats spÃ©cifiques â†’ types prÃ©cis
 * 3. Patterns regex â†’ types optimisÃ©s
 * 4. Fallback â†’ TEXT (universel)
 *
 * @param {any} value - Valeur Ã  analyser
 * @returns {string} - Type PostgreSQL (UUID, INTEGER, BOOLEAN, etc.)
 */
function guessPostgresType(value) {
  // Cas 1: Valeurs nulles ou vides
  if (value === null || value === undefined || value === '') {
    return 'TEXT'; // Type par dÃ©faut pour les valeurs manquantes
  }

  const str = String(value).trim();

  // Cas 2: BoolÃ©ens (diffÃ©rentes notations acceptÃ©es)
  const booleanValues = ['true', 'false', '1', '0', 'yes', 'no', 'y', 'n'];
  if (booleanValues.includes(str.toLowerCase())) {
    return 'BOOLEAN';
  }

  // Cas 3: UUID (format standard UUID v4)
  // Ex: "d47ec620-2190-11ef-910c-f1df4955273f"
  const uuidPattern = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i;
  if (uuidPattern.test(str)) {
    return 'UUID'; // Type natif PostgreSQL, plus efficace que TEXT
  }

  // Cas 4: Entiers (nÃ©gatifs ou positifs)
  // Ex: "-123", "456", "0"
  if (/^-?\d+$/.test(str)) {
    return 'INTEGER';
  }

  // Cas 5: DÃ©cimaux (avec point dÃ©cimal)
  // Ex: "123.45", "-67.89"
  if (/^-?\d+\.\d+$/.test(str)) {
    return 'NUMERIC(12,2)'; // PrÃ©cision fixe pour les prix, montants
  }

  // Cas 6: Dates au format ISO (YYYY-MM-DD)
  // Ex: "2024-06-03"
  if (/^\d{4}-\d{2}-\d{2}$/.test(str)) {
    return 'DATE'; // Date sans heure
  }

  // Cas 7: Dates avec heure (format ISO 8601)
  // Ex: "2024-06-03T10:05:39.625Z", "2024-06-03T10:05:39"
  if (/^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}/.test(str)) {
    return 'TIMESTAMPTZ'; // Date + heure + timezone
  }

  // Cas 8: Par dÃ©faut - texte (compatible avec tout)
  return 'TEXT';
}

/**
 * ğŸ›¡ï¸ Ã‰CHAPPÃ‰MENT DES VALEURS POUR SQL
 * ====================================
 * Convertit une valeur JavaScript en format SQL sÃ»r
 *
 * RÃˆGLES D'Ã‰CHAPPÃ‰MENT :
 * ====================
 * - NULL/undefined/vide â†’ NULL SQL
 * - BOOLEAN â†’ TRUE/FALSE SQL (converti depuis plusieurs formats)
 * - Nombres â†’ valeur numÃ©rique directe (validation NaN)
 * - Textes â†’ entre apostrophes, avec Ã©chappement des apostrophes
 *
 * @param {any} value - Valeur Ã  Ã©chapper
 * @param {string} type - Type PostgreSQL de la colonne
 * @returns {string} - Valeur formatÃ©e pour SQL
 */
function escapeSQLValue(value, type) {
  // Cas 1: Valeurs nulles ou manquantes
  if (value === null || value === undefined || value === '') {
    return 'NULL';
  }

  const str = String(value);

  // Cas 2: BoolÃ©ens (conversion depuis plusieurs formats)
  if (type === 'BOOLEAN') {
    const lower = str.toLowerCase();
    if (['true', '1', 'yes', 'y'].includes(lower)) return 'TRUE';
    if (['false', '0', 'no', 'n'].includes(lower)) return 'FALSE';
    return 'NULL'; // Format non reconnu
  }

  // Cas 3: Nombres (entiers et dÃ©cimaux)
  if (type === 'INTEGER' || type.startsWith('NUMERIC')) {
    const num = parseFloat(str);
    return isNaN(num) ? 'NULL' : String(num);
  }

  // Cas 4: Textes, UUIDs, dates (tous entre apostrophes)
  if (type === 'UUID' || type === 'DATE' || type === 'TIMESTAMPTZ' || type === 'TEXT') {
    // Ã‰chapper les apostrophes : ' â†’ ''
    const escaped = str.replace(/'/g, "''");
    return `'${escaped}'`;
  }

  // Cas 5: Type non gÃ©rÃ© (fallback)
  return 'NULL';
}

/**
 * ğŸ“„ PARSING DES FICHIERS CSV FIBERY
 * ===================================
 * Lit et analyse un fichier CSV exportÃ© depuis Fibery
 *
 * FORMAT ATTENDU :
 * ===============
 * - SÃ©parateur : virgule (,)
 * - Guillemets : doubles pour les valeurs contenant des virgules
 * - Encodage : UTF-8 (support des caractÃ¨res spÃ©ciaux)
 * - Ligne 1 : en-tÃªtes de colonnes
 * - Lignes 2+ : donnÃ©es
 *
 * @param {string} filePath - Chemin vers le fichier CSV
 * @returns {Object} - { headers: [...], rows: [...] }
 */
function parseCSV(filePath) {
  const content = fs.readFileSync(filePath, 'utf8');
  const lines = content.split('\n').filter(line => line.trim());

  if (lines.length === 0) return { headers: [], rows: [] };

  // Ã‰tape 1: Parser la ligne d'en-tÃªte
  const headers = parseCSVLine(lines[0]);

  // Ã‰tape 2: Parser les lignes de donnÃ©es
  const rows = [];
  for (let i = 1; i < lines.length; i++) {
    const values = parseCSVLine(lines[i]);
    if (values.length === headers.length) {
      const row = {};
      headers.forEach((header, idx) => {
        row[header] = values[idx];
      });
      rows.push(row);
    }
  }

  return { headers, rows };
}

/**
 * ğŸ”§ PARSING D'UNE LIGNE CSV
 * ==========================
 * Analyse une ligne CSV en gÃ©rant correctement les guillemets
 *
 * GESTION DES GUILLEMETS :
 * =======================
 * - "valeur" : valeur simple
 * - "valeur avec,virgule" : virgule Ã  l'intÃ©rieur des guillemets
 * - "valeur avec""guillemets" : guillemets doublÃ©s Ã  l'intÃ©rieur
 * - valeur,sans,guillemets : valeurs multiples
 *
 * @param {string} line - Ligne CSV Ã  parser
 * @returns {string[]} - Tableau des valeurs
 */
function parseCSVLine(line) {
  const values = [];
  let current = '';
  let inQuotes = false;

  for (let i = 0; i < line.length; i++) {
    const char = line[i];
    const nextChar = line[i + 1];

    // Cas 1: DÃ©but de guillemets
    if (char === '"' && !inQuotes) {
      inQuotes = true;

    // Cas 2: Guillemets doublÃ©s (Ã©chappement)
    } else if (char === '"' && inQuotes && nextChar === '"') {
      current += '"';  // Ajouter un seul guillemet
      i++; // Sauter le guillemet suivant

    // Cas 3: Fin de guillemets
    } else if (char === '"' && inQuotes) {
      inQuotes = false;

    // Cas 4: SÃ©parateur (virgule) en dehors des guillemets
    } else if (char === ',' && !inQuotes) {
      values.push(current.trim());
      current = '';

    // Cas 5: CaractÃ¨re normal
    } else {
      current += char;
    }
  }

  // Ajouter la derniÃ¨re valeur
  values.push(current.trim());
  return values;
}

// ============================================
// GÃ‰NÃ‰RATION SQL
// ============================================

/**
 * GÃ©nÃ¨re le SQL pour une base de donnÃ©es
 */
function generateSQLForDatabase(dbFolder, dbName) {
  console.log(`\nğŸ“Š Traitement: ${dbName}`);
  
  // Trouver le fichier CSV principal
  const files = fs.readdirSync(dbFolder);
  const csvFile = files.find(f => f.endsWith('.csv'));
  
  if (!csvFile) {
    console.log(`  âš ï¸  Aucun fichier CSV trouvÃ©, skip`);
    return '';
  }
  
  const csvPath = path.join(dbFolder, csvFile);
  console.log(`  â†’ CSV: ${csvFile}`);
  
  // Parser le CSV
  const { headers, rows } = parseCSV(csvPath);
  
  if (headers.length === 0 || rows.length === 0) {
    console.log(`  âš ï¸  CSV vide, skip`);
    return '';
  }
  
  console.log(`  â†’ ${headers.length} colonnes, ${rows.length} lignes`);
  
  // Nettoyer le nom de la table
  const tableName = sanitizeName(dbName);
  console.log(`  â†’ Table SQL: ${CONFIG.schema}.${tableName}`);
  
  // DÃ©tecter les types de colonnes
  const columns = {};
  const hasIdColumn = headers.some(h => sanitizeName(h) === 'id');
  
  for (const header of headers) {
    const cleanHeader = sanitizeName(header);
    
    // Ã‰chantillonner 10 valeurs pour deviner le type
    const sampleValues = rows.slice(0, 10).map(row => row[header]);
    const types = sampleValues.map(v => guessPostgresType(v));
    
    // Type majoritaire
    const typeCounts = {};
    types.forEach(t => typeCounts[t] = (typeCounts[t] || 0) + 1);
    const majorityType = Object.keys(typeCounts).reduce((a, b) => 
      typeCounts[a] > typeCounts[b] ? a : b
    );
    
    columns[header] = {
      sqlName: cleanHeader,
      type: majorityType
    };
  }
  
  // GÃ©nÃ©rer le SQL
  let sql = '';
  
  // CREATE TABLE
  sql += `-- â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n`;
  sql += `-- Table: ${tableName}\n`;
  sql += `-- Source: ${dbName}\n`;
  sql += `-- Lignes: ${rows.length}\n`;
  sql += `-- â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n`;
  
  sql += `DROP TABLE IF EXISTS ${CONFIG.schema}.${tableName} CASCADE;\n\n`;
  
  sql += `CREATE TABLE ${CONFIG.schema}.${tableName} (\n`;
  
  // Si le CSV a dÃ©jÃ  une colonne "Id", on l'utilise comme PK
  // Sinon, on crÃ©e un id auto-gÃ©nÃ©rÃ©
  if (hasIdColumn) {
    // L'Id Fibery sera la clÃ© primaire
    sql += `  -- Colonnes (Id Fibery = Primary Key)\n`;
  } else {
    // CrÃ©er un id auto-gÃ©nÃ©rÃ©
    sql += `  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n`;
    sql += `  \n`;
    sql += `  -- Colonnes\n`;
  }
  
  // Colonnes
  const columnDefs = Object.entries(columns).map(([original, col]) => {
    // Si c'est la colonne Id et qu'elle existe, la marquer comme PK
    if (hasIdColumn && col.sqlName === 'id') {
      return `  ${col.sqlName} UUID PRIMARY KEY`;
    }
    return `  ${col.sqlName} ${col.type}`;
  });
  sql += columnDefs.join(',\n');
  
  // Ajout des colonnes de mÃ©tadonnÃ©es uniquement si pas d'Id Fibery
  if (!hasIdColumn) {
    sql += ',\n';
    sql += `  \n`;
    sql += `  -- MÃ©tadonnÃ©es\n`;
    sql += `  created_at TIMESTAMPTZ DEFAULT NOW(),\n`;
    sql += `  updated_at TIMESTAMPTZ DEFAULT NOW()\n`;
  } else {
    sql += '\n';
  }
  
  sql += `);\n\n`;
  
  // Commentaire
  sql += `COMMENT ON TABLE ${CONFIG.schema}.${tableName} IS 'MigrÃ© depuis Fibery: ${dbName} | ${rows.length} lignes';\n\n`;
  
  // INSERT par batch
  console.log(`  â†’ GÃ©nÃ©ration des INSERT (batch size: ${CONFIG.batchSize})...`);
  
  for (let i = 0; i < rows.length; i += CONFIG.batchSize) {
    const batch = rows.slice(i, i + CONFIG.batchSize);
    
    sql += `-- Batch ${Math.floor(i / CONFIG.batchSize) + 1}/${Math.ceil(rows.length / CONFIG.batchSize)}\n`;
    sql += `INSERT INTO ${CONFIG.schema}.${tableName} (`;
    
    // Noms de colonnes
    const colNames = Object.values(columns).map(c => c.sqlName);
    sql += colNames.join(', ');
    sql += `) VALUES\n`;
    
    // Valeurs
    const valueLines = batch.map(row => {
      const values = Object.entries(columns).map(([original, col]) => {
        return escapeSQLValue(row[original], col.type);
      });
      return `  (${values.join(', ')})`;
    });
    
    sql += valueLines.join(',\n');
    sql += ';\n\n';
  }
  
  console.log(`  âœ… ${rows.length} lignes gÃ©nÃ©rÃ©es`);
  
  return sql;
}

// ============================================
// MAIN
// ============================================

function main() {
  console.log('â”'.repeat(80));
  console.log('ğŸš€ CSV TO SQL MIGRATOR - Fibery â†’ Supabase');
  console.log('â”'.repeat(80));
  console.log(`ğŸ“‚ Dossier source: ${CONFIG.importantDir}`);
  console.log(`ğŸ“„ Fichier SQL: ${CONFIG.outputSQL}`);
  console.log(`ğŸ—‚ï¸  SchÃ©ma PostgreSQL: ${CONFIG.schema}`);
  console.log('â”'.repeat(80));
  
  // VÃ©rifier que le dossier existe
  if (!fs.existsSync(CONFIG.importantDir)) {
    console.error(`âŒ ERREUR: Dossier introuvable: ${CONFIG.importantDir}`);
    console.log('\nğŸ’¡ Modifiez la variable importantDir dans le script');
    process.exit(1);
  }
  
  // Lister les sous-dossiers (bases de donnÃ©es)
  const databases = fs.readdirSync(CONFIG.importantDir)
    .filter(name => {
      const fullPath = path.join(CONFIG.importantDir, name);
      return fs.statSync(fullPath).isDirectory();
    })
    .sort();
  
  console.log(`\nğŸ“Š ${databases.length} bases de donnÃ©es trouvÃ©es:\n`);
  databases.forEach((db, idx) => {
    console.log(`  ${idx + 1}. ${db}`);
  });
  
  // GÃ©nÃ©rer le header SQL
  let fullSQL = '';
  fullSQL += `-- â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n`;
  fullSQL += `-- MIGRATION FIBERY â†’ SUPABASE\n`;
  fullSQL += `-- GÃ©nÃ©rÃ© le: ${new Date().toISOString()}\n`;
  fullSQL += `-- Bases: ${databases.length}\n`;
  fullSQL += `-- â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n`;
  fullSQL += `-- CrÃ©ation du schÃ©ma\n`;
  fullSQL += `CREATE SCHEMA IF NOT EXISTS ${CONFIG.schema};\n`;
  fullSQL += `SET search_path TO ${CONFIG.schema}, public;\n\n`;
  fullSQL += `-- Extension UUID\n`;
  fullSQL += `CREATE EXTENSION IF NOT EXISTS "uuid-ossp";\n\n`;
  
  // Traiter chaque base
  for (const db of databases) {
    const dbFolder = path.join(CONFIG.importantDir, db);
    const sql = generateSQLForDatabase(dbFolder, db);
    fullSQL += sql;
  }
  
  // Footer
  fullSQL += `-- â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n`;
  fullSQL += `-- FIN DE LA MIGRATION\n`;
  fullSQL += `-- â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n`;
  fullSQL += `RESET search_path;\n`;
  
  // Sauvegarder
  fs.writeFileSync(CONFIG.outputSQL, fullSQL, 'utf8');
  
  const fileSize = (fs.statSync(CONFIG.outputSQL).size / 1024).toFixed(1);
  
  console.log('\nâ”'.repeat(80));
  console.log('âœ… MIGRATION SQL GÃ‰NÃ‰RÃ‰E !');
  console.log('â”'.repeat(80));
  console.log(`ğŸ“„ Fichier: ${CONFIG.outputSQL}`);
  console.log(`ğŸ“Š Taille: ${fileSize} KB`);
  console.log(`\nğŸš€ PROCHAINES Ã‰TAPES:`);
  console.log(`  1. Ouvrez Supabase SQL Editor`);
  console.log(`  2. Copiez-collez le contenu de ${CONFIG.outputSQL}`);
  console.log(`  3. Cliquez "Run" pour exÃ©cuter`);
  console.log('â”'.repeat(80));
}

// Lancer le programme
try {
  main();
} catch (error) {
  console.error('\nâŒ ERREUR:', error.message);
  console.error(error.stack);
  process.exit(1);
}